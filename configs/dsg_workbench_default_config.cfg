[configs]
## Files in test_file_dir will be used to determine the human tagging while computing SSI classification accuracy. Files should be named as test_<FOLD_NUMBER>. For ex. the test file name for fold 1 should be test_1
## Eg:- test_file_dir=/home/jithin.j/ebay/ssi1/Folds
test_file_dir=../test/

## Files in asr_output_dir will be used as an input to the SSI module, IF ASR is set to false. In asr_output_dir, files should be named as ASR_output_<FOLD_NUMBER>. For ex. ASR output for fold 1 should be named as ASR_o
##Eg:- asr_output_dir=/home/jithin.j/ebay/ssi1/ASROutput
asr_output_dir=./test/

## Location of the audio files. Only URLs should be used for specifying audio file locations. A leading forward slash should ALWAYS be included in the utterance_dir name (as below).
## Eg:- utterance_dir=http://anvil.tellme.com/~sjoshi/Audio/ebay65k/allAudio/
utterance_dir=

##the output folder path
##Eg:- output_folder=/home/jithin.j/ebay/ssi1/Results
output_folder=../Results

##Stable DNN
webreco_url=http://stable-webreco.voice.lb-priv.sv2.247-inc.net/reco

##client id & shared key for dnn stable version
client_id=247inc-dsg-dnn
sharedKey=mN8PaFSHqIf$dt?{PAPR

#The proxy to be used for recognition expt,use webproxy if running from your laptop, from servers use cache.backside
proxy=cache.backside.sv2.tellme.com
#proxy=webproxy.cell.sv2.tellme.com

n_best=1
acoustic_model=en-us.dnn
reco_confidence_level=0.0
speedvsaccuracy=0.5
sensitivity=0.5
parallel_recos=10

## Flag for performing recognition. If ASR (Automatic Speech Recognition) is set to False, workbench will use files in the asr_output_dir as input for SSI.
ASR=false
SSI=true
##This option should be set to true in case of SLM+SSI experiment if you want to take classified intents from the recogntion output. Even if this option is set to true workbench still needs the original SSI model as input(the list of intents is taken from the SSI model)
##If this option is set, an additional column called "Final_processed_string" will be present in the classification output which is the string with normalizations applied on it. This is the string used just before the classification step.
use_classification_output_from_reco=false

## Files in slm_model_dir will be used as grammars for performing experiments. SLM models should always be specified as URLs.
## Naming convention: In slm_model_dir, SLM+SSI compiled files should be named as Fold_<FOLD_NUMBER>.cfg. For ex. model to be used for fold 1 should be named Fold_1.cfg. Also, if use_grxml_in_parallel=true, then a standard GRXML file should be present in the slm_model_dir folder. The grxml file should be named as standard_grammar.grxml. A leading forward slash should ALWAYS be included in the slm_model_dir name (as below).
## Eg:- slm_model_dir=http://anvil.tellme.com/~jgeorge/ebay/slm_ssi/gb/slm_full/
slm_model_dir=

## Files in ssi_model_dir will be used as SSI models. In ssi_model_dir, files should be named as ssi_<FOLD_NUMBER>. For ex. SSI model to be used for fold 1 should be named ssi_1
##Eg:- ssi_model_dir=/home/jithin.j/ebay/ssi1/ssi_models
ssi_model_dir=

### Algorithm to be used for testing. Options:
## svm Support Vector Machines
## mnb (Multinomial Naive Bayes)
## lsi (Latent Semantic Analysis with Cosine Similarity)
## vsc (Vector Space (original space) with Cosine Similarity)
## tfidfnb (Naive Bayes with tf-Idf) ** not tested in workbench yet **
## twcnb (Transformed Weight Normalized Complementary Naive Bayes) ** Not included in workbench **
training_algorithm=svm
# kernel: linear (default), poly, radial
svm_kern_kernel=linear
svm_kern_poly_deg=2.0
svm_kern_radial_gamma=1.0

## use_wrapper_grammar should be set to true if the static grammar and SLM are enclosed within a wrapper grammar. GRXML files named as cfg_wrapper_<fold_number>.grxml should be placed in the slm_model_dir.
use_wrapper_grammar=false

## use_grxml_in_parallel should be set to true if a static grammar and SLM are to be used as top-level grammars. A .grxml file named standard_grammar.grxml should be saved in slm_model_dir.
use_grxml_in_parallel=false
## slm_weight should be set, if a static grammar and SLM are being used as top-level grammars. Weight of the standard grammar will be calculated as 1.0-slm_weight.
slm_weight=0.001
## If a static grammar and SLM are being used as top-level grammars, then order of SLM/static grammar in the webreco requests can be controlled using slm_position. For ex, if slm should be mentioned before the static grammar, slm_poition=first. If static grammar should be mentioned first in the webreco request, then slm_position=second
slm_position=second

## Recognition score threshold on the static grammar when it is used within a wrapper grammar or a top-level grammar.
grxml_threshold=0.7

### Features type. Options:
## word (only word based features)
## all (words, n-gram, AND rules)
feature_level=all
stemming=true
stopword_removal=true

perform_WER_on_class_data=true

remove_infrequent_feats=false
min_df=1.0

## limit the scope of AND features to a distance of feat_and_window words. 0 means no limit
feat_and_window=10

## add wordcount as a feature
feat_wordcount=false

## compute triples for AND rules (co-occurrences), in addition to pairs
feat_and_triples=false

## use_model_stems should be set to false if you want to generate stems (even for new words) during test/classify step using wordnet. 
# In production it is reusing the model stems, so to replicate that keep this option as true. Not a mandatory field
# default_value = true
use_model_stems=true

## keeping replace_word_classes=false for slm+ssi expt will result in the use of the word class replaced string from reco output as the input string for ssi expt
# currently this replacement happens before providing input data to workbench. Not a mandatory field
# default value= false.
replace_word_classes=false
use_sentence_marker=true

## This is to merge consecutive duplicates like "this this".This is done after stopword removal,because the duplicate words can be separated by some stop word eg: "this uh um this"
## Note:- this will note replace consecutive occurances of _class_ Eg: "_class_number _class_number" will not be merged into one. This was done to match web2nl behaviour
merge_consecutive_duplicate_word=true

##This is to merge consecutive duplicate word classes. Eg: "_class_number _class_number" will be replaced with "_class_number".
##This option should be kept as false for the final web2nl model as this operation is currently not handled in web2nl. This option was added only for experimentation purposes.
merge_consecutive_duplicate_class=false

##This parameter is used to reduce the precision for the feature weights, this will help reduce the model size.
## Specify how many digits you want to retain after the decimal places
feature_precision=5

### IF use_model_stems is set to true, the following text processing files and options will NOT BE USED.
### Files required for text preprocessing.
stopwords_file=supporting_files/stopwords.txt

##deprecated location class replacement is done along with other word classes
location_class_file=supporting_files/location_class.txt

##this word class replacement is also now done as part of before giving input to workbench, as part of the normalization script
word_classes_file=supporting_files/word_classes.txt
word_expansion_file=supporting_files/Apostrophe_words.txt
wordnet_properties_file=supporting_files/file_properties.xml
stemming_exceptions_file=supporting_files/stemming_exceptions.txt
##########################################################################################################
##########################################################################################################
#################END OF WORKBENCH PARAMS##################################################################
##########################################################################################################
##########################################################################################################


## Training data file when used in SSI 'train' mode
## Eg:- data_file=/home/jithin.j/ebay/ssi1/FirstResponses_WCNormalized
data_file=input/FirstResponses_WCNormalized
#data_file=/Users/huzefa.siyamwala/PycharmProjects/cleaner/src/normalized_data/FirstResponses_WCNormalized

## Test file when used in 'SSI' test mode
## Eg: - test_file=/home/jithin.j/ebay/ssi1/FirstResponses_WCNormalized
test_file=./test/test_1

## SSI model location when used in SSI 'test' mode
ssi_model=

## Number of folds to be used for cross-validation
number_of_folds=2
## Add stem inflections which aren't present in the training data. Used for chat/text NL models.
# This will check for inflections of a word & add it only if their stem exists & if stem is the same as the root word
# Eg:- let's say "explain" have "explained","explaining","explains" as inflections,
# it will keep {"explained":"explain","explaining":"explain","explains":"explain"} as a stems (since explain is the stem of all these versions)
# Note:- if "explained" occured instead of "explain" it wouldn't added any entries as "explained" doesn't have inflections
add_stem_inflections=false
#agid_inflection_file_path=./supporting_files/infl.txt
agid_inflection_file_path=/var/tellme/modelbuilder_worker/supporting_files/infl.txt

## if you want to take inflections of a stem as well, keep this option as true
# Eg:- let's say - "explained" have stem=explain. It's stem "explain" have inflections. 
# This option will take explain's inflections as new entries (if their stem=explain)
# i.e {"explained":"explain","explaining":"explain","explains":"explain"}
# previous version of workbench used to had this enabled due to a bug.
# keeping this configurable in case we want to experiment with this. This option would work only if add_stem_inflections=true as well
# default : true (not mandatory field)
include_stems_inflections=true

## Parameters for training Sigmoid functions for each intent. This will increase the model training time considerably
use_sgmd_normalization=true
number_of_folds_Sgmd_Training=2

## SVM training parameters
##Eg:- svm_light_dir=/home/jithin.j/.workbench/SSI-1.0/resources/svm_light
svm_light_dir=/var/tellme/workbench/SSI-1.0/resources/svm_light
#svm_light_dir=/Users/huzefa.siyamwala/.workbench/SSI-1.0/resources/svm_light
svm_error_margin_tradeoff=1.0
grid_min=0.0
grid_max=0.1
search_interval=0.01

## This is the misclassfication cost for classes whose count is less than 0.1 percentage of total data
svm_misclassification_cost=1

## number of times each minority class instance would be duplicated. A class/intent is considered minority if it occurs <1% in the dataset
min_class_instance_replacement=25

## number of parallel binary SVM (svmlight) trainings to perform
parallel_SVM_trainings=10

## number of parallel sigmoid model trainings to perform
parallel_sigmoid_trainings=20

## This value determines how much of the main code can run simultaneously, 
# i.e running main train & sigmoid folds expt, sigmoid svm models simultaneously
# default value = -1 & signifies a dynamic setting where there is not limit
# This value shouldn't be modified (either don't specify or keep default), 
# unless you run into out of memory issues
parallel_executions=-1
ASR_output=false

### train_with_CV (perform cross validation on the training set to find the best model parameters)
### train_with_CV,train (run both train experiment & train_with_CV experiment, specified order doesn't matter)
### train (train model using the specified modeling technique)
### train_hierarchy_with_cv for hierarchical train with cv experiment
### test (use the serialized training model for testing the test set)
### test_hierarchy
### grid_search (applicable only for SVMs)
### grid_search_new (if you want to run the scikit learn based gridsearch experiment)
experiment_type=train_with_cv

##Intent hierarchy file - This is the file mentioning the hierarchy of the intents. not required for normal expt
#intent_hierarchy_file=/home/jithin.j/IBM/ssi10_hierarchical_full/SupportingFiles/Intent_Hierarchy.txt

## Use already created training and test folds instead of creating new ones
use_predefined_cv_folds=false
## Eg:- cv_fold_directory=/home/jithin.j/ebay/ssi1/Folds/
cv_fold_directory=../folds/

## out of domain intent
out_of_domain_intent=None_None

## Perform feature selection or not. Feature selection is performed using bi-normal separation scores. Feature selection requires significant computational resources - set max heap space to at least 6G.
feature_selection=false
## Percentage of top/bottom features to return. Setting it to 50 will retain 25% top and bottom features for SSI model training, based on their BNS scores.
num_feats_to_retain=100

## Deletes SVM training files - reduces the disk utilization considerably
remove_data_files=true

## Deletes each intent wise SVM models created & logs associated with svm light
# this will save some space & will save some time for writing logs
## This can be kept as false if you need to do some debugging
# Default value = true (not mandatory field)
remove_intermediate_svm_files=true

###temporary directory for running the jobs
##if the temporary folder does not exist, experiment will be run in the regular output folder, 
##else intermediate output is saved in temp_dir
##if this parameter is not specified a default value is picked up
## default value - /var/extra/tmp/workbench
temp_dir=/var/tellme/extra/workbench

##
###reporting params
##

## The input data can be from various sources
# Some of this data is internal data & we don't want to expose this in
# the external report
report_types={"internal":["I","A","E"],"external":["E"]}

## name for the internal report, in above specified report_types
# this will be used for creating files symlink file without filetypes (to maintain backward compatibility)
internal_report_name=internal

##value for internal row type
## this value will be used if rowType is not specified
internal_row_type=I

####
####params for new grid search module using scikit-learn
##min value for c param
c_min=0.1
##multiplication factor to decide next values of c
c_multiplication_factor=10
##total number of entries to generate for c
c_entries=5

##scoring metric for gridsearch code in sklearn
#for valid params check https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
scoring_metric=f1_macro
##if you have params that you want to pass to sklearn code provide that in a comma separated form here
##Eg:-sklearn_params=c_min,c_multiplication_factor,c_entries,scoring_metric
sklearn_params=

###python3 executable location
##python=/usr/local/bin/python3
#python=/Users/huzefa.siyamwala/Code/huzefa/orion/env/bin/python3.6
python = /var/tellme/modelbuilder_worker/orionEnv/bin/python3.6

###python scripts directory,which contains the gridsearch code
##python_scripts_folder=/home/jithin.j/.workbench/SSI-1.0/resources/python
#python_scripts_folder=/Users/huzefa.siyamwala/Code/dsg/workbench//SSI-1.0/resources/python
python_scripts_folder=/var/tellme/workbench/workspace/DSG-huzefa-siyamwala/statistical-semantic-interpretter/SSI-1.0/resources/python